{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc4eeef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from tensorflow.python import keras \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7bb14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# loading data and opening our input data in the form of a text file\n",
    "#Project Gutenburg/berg\n",
    "file = open(\"frankenstein.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "11e85992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "# standardization\n",
    "def tokenize_words(input):\n",
    "    input = input.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dee61521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chars to number\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "69cd1d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 269566\n",
      "Total vocab: 38\n"
     ]
    }
   ],
   "source": [
    "#check if words to chars or chars to num (?!) has worked?\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print(\"Total number of characters:\", input_len)\n",
    "print(\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4c60d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq length\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "60f7f505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 269466\n"
     ]
    }
   ],
   "source": [
    "# loop through the sequence\n",
    "for i in range (0, input_len - seq_length,1):\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "    \n",
    "n_patterns = len(x_data)\n",
    "print(\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2b951e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input sequence to np arrary and so on\n",
    "X = numpy.reshape(x_data,(n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "26c84d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "y = utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1e56afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "model =  Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7ba7107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the  model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5da05f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving weights\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose = 1, save_best_only = True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "206e1042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1053/1053 [==============================] - 100s 92ms/step - loss: 2.4048\n",
      "\n",
      "Epoch 00001: loss improved from 2.47797 to 2.38557, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 2.3273\n",
      "\n",
      "Epoch 00002: loss improved from 2.38557 to 2.30750, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/50\n",
      "1053/1053 [==============================] - 95s 91ms/step - loss: 2.2525\n",
      "\n",
      "Epoch 00003: loss improved from 2.30750 to 2.23702, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/50\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 2.1808\n",
      "\n",
      "Epoch 00004: loss improved from 2.23702 to 2.17440, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/50\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 2.1355\n",
      "\n",
      "Epoch 00005: loss improved from 2.17440 to 2.12601, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 2.0878\n",
      "\n",
      "Epoch 00006: loss improved from 2.12601 to 2.08226, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 2.0520\n",
      "\n",
      "Epoch 00007: loss improved from 2.08226 to 2.04852, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 2.0164\n",
      "\n",
      "Epoch 00008: loss improved from 2.04852 to 2.01616, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/50\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 1.9933\n",
      "\n",
      "Epoch 00009: loss improved from 2.01616 to 1.99033, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/50\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 1.9684\n",
      "\n",
      "Epoch 00010: loss improved from 1.99033 to 1.96747, saving model to model_weights_saved.hdf5\n",
      "Epoch 11/50\n",
      "1053/1053 [==============================] - 98s 93ms/step - loss: 1.9434\n",
      "\n",
      "Epoch 00011: loss improved from 1.96747 to 1.94553, saving model to model_weights_saved.hdf5\n",
      "Epoch 12/50\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 1.9326\n",
      "\n",
      "Epoch 00012: loss improved from 1.94553 to 1.92574, saving model to model_weights_saved.hdf5\n",
      "Epoch 13/50\n",
      "1053/1053 [==============================] - 99s 94ms/step - loss: 1.9097\n",
      "\n",
      "Epoch 00013: loss improved from 1.92574 to 1.90960, saving model to model_weights_saved.hdf5\n",
      "Epoch 14/50\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 1.8896\n",
      "\n",
      "Epoch 00014: loss improved from 1.90960 to 1.89128, saving model to model_weights_saved.hdf5\n",
      "Epoch 15/50\n",
      "1053/1053 [==============================] - 100s 95ms/step - loss: 1.8727\n",
      "\n",
      "Epoch 00015: loss improved from 1.89128 to 1.87712, saving model to model_weights_saved.hdf5\n",
      "Epoch 16/50\n",
      "1053/1053 [==============================] - 98s 93ms/step - loss: 1.8583\n",
      "\n",
      "Epoch 00016: loss improved from 1.87712 to 1.86299, saving model to model_weights_saved.hdf5\n",
      "Epoch 17/50\n",
      "1053/1053 [==============================] - 98s 93ms/step - loss: 1.8479\n",
      "\n",
      "Epoch 00017: loss improved from 1.86299 to 1.85076, saving model to model_weights_saved.hdf5\n",
      "Epoch 18/50\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 1.8345\n",
      "\n",
      "Epoch 00018: loss improved from 1.85076 to 1.83717, saving model to model_weights_saved.hdf5\n",
      "Epoch 19/50\n",
      "1053/1053 [==============================] - 96s 92ms/step - loss: 1.8265\n",
      "\n",
      "Epoch 00019: loss improved from 1.83717 to 1.82735, saving model to model_weights_saved.hdf5\n",
      "Epoch 20/50\n",
      "1053/1053 [==============================] - 95s 91ms/step - loss: 1.8155\n",
      "\n",
      "Epoch 00020: loss improved from 1.82735 to 1.81731, saving model to model_weights_saved.hdf5\n",
      "Epoch 21/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.8084\n",
      "\n",
      "Epoch 00021: loss improved from 1.81731 to 1.80802, saving model to model_weights_saved.hdf5\n",
      "Epoch 22/50\n",
      "1053/1053 [==============================] - 95s 91ms/step - loss: 1.7955\n",
      "\n",
      "Epoch 00022: loss improved from 1.80802 to 1.79844, saving model to model_weights_saved.hdf5\n",
      "Epoch 23/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.7807\n",
      "\n",
      "Epoch 00023: loss improved from 1.79844 to 1.78621, saving model to model_weights_saved.hdf5\n",
      "Epoch 24/50\n",
      "1053/1053 [==============================] - 95s 91ms/step - loss: 1.7774\n",
      "\n",
      "Epoch 00024: loss improved from 1.78621 to 1.78101, saving model to model_weights_saved.hdf5\n",
      "Epoch 25/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.7659\n",
      "\n",
      "Epoch 00025: loss improved from 1.78101 to 1.77232, saving model to model_weights_saved.hdf5\n",
      "Epoch 26/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.7597\n",
      "\n",
      "Epoch 00026: loss improved from 1.77232 to 1.76221, saving model to model_weights_saved.hdf5\n",
      "Epoch 27/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.7568\n",
      "\n",
      "Epoch 00027: loss improved from 1.76221 to 1.75993, saving model to model_weights_saved.hdf5\n",
      "Epoch 28/50\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 1.7450\n",
      "\n",
      "Epoch 00028: loss improved from 1.75993 to 1.75039, saving model to model_weights_saved.hdf5\n",
      "Epoch 29/50\n",
      "1053/1053 [==============================] - 95s 91ms/step - loss: 1.7368\n",
      "\n",
      "Epoch 00029: loss improved from 1.75039 to 1.74316, saving model to model_weights_saved.hdf5\n",
      "Epoch 30/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.7315\n",
      "\n",
      "Epoch 00030: loss improved from 1.74316 to 1.73378, saving model to model_weights_saved.hdf5\n",
      "Epoch 31/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.7304\n",
      "\n",
      "Epoch 00031: loss improved from 1.73378 to 1.73144, saving model to model_weights_saved.hdf5\n",
      "Epoch 32/50\n",
      "1053/1053 [==============================] - 96s 92ms/step - loss: 1.7224\n",
      "\n",
      "Epoch 00032: loss improved from 1.73144 to 1.72574, saving model to model_weights_saved.hdf5\n",
      "Epoch 33/50\n",
      "1053/1053 [==============================] - 97s 92ms/step - loss: 1.7164\n",
      "\n",
      "Epoch 00033: loss improved from 1.72574 to 1.71983, saving model to model_weights_saved.hdf5\n",
      "Epoch 34/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.7098\n",
      "\n",
      "Epoch 00034: loss improved from 1.71983 to 1.71389, saving model to model_weights_saved.hdf5\n",
      "Epoch 35/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.7083\n",
      "\n",
      "Epoch 00035: loss improved from 1.71389 to 1.70740, saving model to model_weights_saved.hdf5\n",
      "Epoch 36/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.7038\n",
      "\n",
      "Epoch 00036: loss improved from 1.70740 to 1.70538, saving model to model_weights_saved.hdf5\n",
      "Epoch 37/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6978\n",
      "\n",
      "Epoch 00037: loss improved from 1.70538 to 1.69934, saving model to model_weights_saved.hdf5\n",
      "Epoch 38/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6895\n",
      "\n",
      "Epoch 00038: loss improved from 1.69934 to 1.69336, saving model to model_weights_saved.hdf5\n",
      "Epoch 39/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6892\n",
      "\n",
      "Epoch 00039: loss improved from 1.69336 to 1.69049, saving model to model_weights_saved.hdf5\n",
      "Epoch 40/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6772\n",
      "\n",
      "Epoch 00040: loss improved from 1.69049 to 1.68384, saving model to model_weights_saved.hdf5\n",
      "Epoch 41/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6794\n",
      "\n",
      "Epoch 00041: loss improved from 1.68384 to 1.67944, saving model to model_weights_saved.hdf5\n",
      "Epoch 42/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6736\n",
      "\n",
      "Epoch 00042: loss improved from 1.67944 to 1.67672, saving model to model_weights_saved.hdf5\n",
      "Epoch 43/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6656\n",
      "\n",
      "Epoch 00043: loss improved from 1.67672 to 1.67065, saving model to model_weights_saved.hdf5\n",
      "Epoch 44/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6669\n",
      "\n",
      "Epoch 00044: loss did not improve from 1.67065\n",
      "Epoch 45/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6546\n",
      "\n",
      "Epoch 00045: loss improved from 1.67065 to 1.66339, saving model to model_weights_saved.hdf5\n",
      "Epoch 46/50\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 1.6556\n",
      "\n",
      "Epoch 00046: loss improved from 1.66339 to 1.66306, saving model to model_weights_saved.hdf5\n",
      "Epoch 47/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6509\n",
      "\n",
      "Epoch 00047: loss improved from 1.66306 to 1.65717, saving model to model_weights_saved.hdf5\n",
      "Epoch 48/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6496\n",
      "\n",
      "Epoch 00048: loss improved from 1.65717 to 1.65526, saving model to model_weights_saved.hdf5\n",
      "Epoch 49/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6483\n",
      "\n",
      "Epoch 00049: loss improved from 1.65526 to 1.65010, saving model to model_weights_saved.hdf5\n",
      "Epoch 50/50\n",
      "1053/1053 [==============================] - 96s 91ms/step - loss: 1.6393\n",
      "\n",
      "Epoch 00050: loss improved from 1.65010 to 1.64759, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f76a4e49130>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model and let it train\n",
    "model.fit(X,y, epochs=50, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dbf16b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompile model with the saved weights\n",
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a51a06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of the model back into the characters\n",
    "num_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "97d651a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed: \n",
      "\" ated lighthearted gaiety boyhood winds whispered soothing accents maternal nature bade weep kindly i \"\n"
     ]
    }
   ],
   "source": [
    "# random seed to help generate\n",
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed: \")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "93f5d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npuired sea could seen searon searon sea considerable searon searon sea considerable searon searon searon sea considerable season searon searon searon searon searon sea considerable season searon searon sea considerable season searon searon searon searon searon sea considerable season searon searon sea considerable season searon searon searon searon searon sea considerable season searon searon sea considerable season searon searon searon searon searon sea considerable season searon searon sea considerable season searon searon searon searon searon sea considerable season searon searon sea considerable season searon searon searon searon searon sea considerable season searon searon sea considerable season searon searon searon searon searon sea considerable season searon searon sea considerable season searon searon searon searon searon sea considerable season searon searon sea considerable season searon searon searon searon searon sea considerable season searon searon sea considerable seas"
     ]
    }
   ],
   "source": [
    "# generate the text\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern,(1,len(pattern), 1))\n",
    "    x = x/float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern =  pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bedc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
